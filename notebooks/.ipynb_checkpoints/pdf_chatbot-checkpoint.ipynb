{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7673c408-0cba-4627-bedf-fb9664d919e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"pdfs\", exist_ok=True)\n",
    "os.makedirs(\"chunks\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc95ec92-d5fd-4b48-826c-f17cdaa2df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        doc.close()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d81bebb-6676-4576-aba7-b781cca4027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import json\n",
    "import re\n",
    "\n",
    "def chunk_and_store_text(text, source_name, chunk_size=1000, chunk_overlap=200):\n",
    "    # Check for empty text\n",
    "    if not text.strip():\n",
    "        print(f\"Warning: Empty text received for {source_name}\")\n",
    "        return []\n",
    "\n",
    "    # Sanitize source name for filenames\n",
    "    safe_source_name = re.sub(r'\\W+', '_', source_name)\n",
    "\n",
    "    # Create text splitter\n",
    "    splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunks = splitter.split_text(text)\n",
    "\n",
    "    # Prepare chunk data with metadata\n",
    "    data = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        data.append({\n",
    "            \"id\": f\"{safe_source_name}_{i}\",\n",
    "            \"text\": chunk,\n",
    "            \"metadata\": {\"source\": safe_source_name}\n",
    "        })\n",
    "\n",
    "    # Write to JSON\n",
    "    with open(f\"chunks/{safe_source_name}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39cea1b6-6d95-44ac-a1c3-0fe7a8fd4b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Processing: ISLP_website.pdf\n",
      "‚úÖ 1828 chunks created from ISLP_website.pdf\n",
      "\n",
      "üìÑ Processing: thinkpython2.pdf\n",
      "‚úÖ 558 chunks created from thinkpython2.pdf\n",
      "\n",
      "üìÑ Processing: Eloquent_JavaScript.pdf\n",
      "‚úÖ 965 chunks created from Eloquent_JavaScript.pdf\n",
      "\n",
      "üìÑ Processing: thinkjava.pdf\n",
      "‚úÖ 478 chunks created from thinkjava.pdf\n",
      "\n",
      "üìÑ Processing: ISLRv2_corrected_June_2023.pdf\n",
      "‚úÖ 1718 chunks created from ISLRv2_corrected_June_2023.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_all_pdfs(pdf_folder=\"pdfs/\"):\n",
    "    all_chunks = []\n",
    "    for file in os.listdir(pdf_folder):\n",
    "        if file.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, file)\n",
    "            print(f\"üìÑ Processing: {file}\")\n",
    "            \n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "            if not text.strip():\n",
    "                print(f\"‚ö†Ô∏è Skipped (empty text): {file}\")\n",
    "                continue\n",
    "\n",
    "            source_name = file.rsplit(\".\", 1)[0]\n",
    "            chunks = chunk_and_store_text(text, source_name=source_name)\n",
    "\n",
    "            print(f\"‚úÖ {len(chunks)} chunks created from {file}\\n\")\n",
    "            all_chunks.extend(chunks)\n",
    "    return all_chunks\n",
    "\n",
    "# Call it\n",
    "all_chunks = process_all_pdfs()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1169aa9-3b31-4c4f-989b-0ac3ec62a14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 11094 chunks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def load_all_chunks(folder=\"chunks/\"):\n",
    "    documents = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.lower().endswith(\".json\"):\n",
    "            file_path = os.path.join(folder, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    documents.extend(data)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading {file}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# Load all chunks\n",
    "documents = load_all_chunks()\n",
    "print(f\"‚úÖ Loaded {len(documents)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a55092a4-8c06-467a-9797-02d9ddc9d93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee121f2ac4004af1b8c258678236d461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# No need to extract \"text\" from docs, since each is already a string\n",
    "embeddings = embedding_model.encode(documents, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ea14d7-85e2-48bc-b2fa-c12285d36467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"pdf_chunks\",\n",
    ")\n",
    "\n",
    "# Normalize: convert everything to plain text strings\n",
    "clean_texts = []\n",
    "metadata_list = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    if isinstance(doc, dict):  # If it's already structured\n",
    "        clean_texts.append(doc[\"text\"])\n",
    "        metadata_list.append(doc.get(\"metadata\", {\"chunk\": i}))\n",
    "    else:  # If it's just a plain string\n",
    "        clean_texts.append(doc)\n",
    "        metadata_list.append({\"chunk\": i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0333561-83fe-4c9a-b955-6d420bb48993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ac16e981124cd7b80661801ff43c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = embedding_model.encode(clean_texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "876d94fc-02fd-473e-b2f9-99735a1f4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(clean_texts):\n",
    "    collection.add(\n",
    "        ids=[f\"chunk_{i}\"],\n",
    "        documents=[text],\n",
    "        embeddings=[embeddings[i]],\n",
    "        metadatas=[metadata_list[i]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9914315c-fe71-4c5e-95c7-c424e0a912d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1:\n",
      "[‚Ä¶] which defines the type in terms of the operations which can be\n",
      "performed on it.‚Äù\n",
      "‚ÄîBarbara Liskov, Programming with Abstract Data Types\n",
      "Chapter 6\n",
      "The Secret Life of Objects\n",
      "Chapter 4 introduced JavaScript‚Äôs objects as containers that hold other data.\n",
      "In programming culture, object-oriented programming is a set of techniques that\n",
      "use objects as the central principle of program organization. Though no one\n",
      "really agrees on its precise definition, object-oriented programming has shaped\n",
      "the design of many programming languages, including JavaScript. This chapter\n",
      "describes the way these ideas can be applied in JavaScript.\n",
      "Abstract Data Types\n",
      "The main idea in object-oriented programming is to use objects, or rather\n",
      "types of objects, as the unit of program organization. Setting up a program\n",
      "as a number of strictly separated object types provides a way to think about\n",
      "its structure and thus to enforce some kind of discipline, preventing everything\n",
      "from becoming entangled.\n",
      "\n",
      "Chunk 2:\n",
      "[‚Ä¶] which defines the type in terms of the operations which can be\n",
      "performed on it.‚Äù\n",
      "‚ÄîBarbara Liskov, Programming with Abstract Data Types\n",
      "Chapter 6\n",
      "The Secret Life of Objects\n",
      "Chapter 4 introduced JavaScript‚Äôs objects as containers that hold other data.\n",
      "In programming culture, object-oriented programming is a set of techniques that\n",
      "use objects as the central principle of program organization. Though no one\n",
      "really agrees on its precise definition, object-oriented programming has shaped\n",
      "the design of many programming languages, including JavaScript. This chapter\n",
      "describes the way these ideas can be applied in JavaScript.\n",
      "Abstract Data Types\n",
      "The main idea in object-oriented programming is to use objects, or rather\n",
      "types of objects, as the unit of program organization. Setting up a program\n",
      "as a number of strictly separated object types provides a way to think about\n",
      "its structure and thus to enforce some kind of discipline, preventing everything\n",
      "from becoming entangled.\n",
      "\n",
      "Chunk 3:\n",
      "procedural programming and transition to object-oriented more gradually.\n",
      "Many of Java‚Äôs object-oriented features are motivated by problems with previ-\n",
      "ous languages, and their implementations are inÔ¨Çuenced by this history. Some\n",
      "of these features are hard to explain when people aren‚Äôt familiar with the\n",
      "problems they solve.\n",
      "We get to object-oriented programming as quickly as possible, limited by the\n",
      "requirement that we introduce concepts one at a time, as clearly as possible,\n",
      "in a way that allows readers to practice each idea in isolation before moving\n",
      "on. So it takes some time to get there.\n",
      "But you can‚Äôt write Java programs (even hello world) without encountering\n",
      "object-oriented features. In some cases we explain a feature brieÔ¨Çy when it\n",
      "Ô¨Årst appears, and then explain it more deeply later on.\n",
      "This book is well suited to prepare students for the AP Computer Science\n",
      "A exam, which includes object-oriented design and implementation. (AP is\n",
      "PREFACE\n",
      "xv\n",
      "\n",
      "Chunk 4:\n",
      "procedural programming and transition to object-oriented more gradually.\n",
      "Many of Java‚Äôs object-oriented features are motivated by problems with previ-\n",
      "ous languages, and their implementations are inÔ¨Çuenced by this history. Some\n",
      "of these features are hard to explain when people aren‚Äôt familiar with the\n",
      "problems they solve.\n",
      "We get to object-oriented programming as quickly as possible, limited by the\n",
      "requirement that we introduce concepts one at a time, as clearly as possible,\n",
      "in a way that allows readers to practice each idea in isolation before moving\n",
      "on. So it takes some time to get there.\n",
      "But you can‚Äôt write Java programs (even hello world) without encountering\n",
      "object-oriented features. In some cases we explain a feature brieÔ¨Çy when it\n",
      "Ô¨Årst appears, and then explain it more deeply later on.\n",
      "This book is well suited to prepare students for the AP Computer Science\n",
      "A exam, which includes object-oriented design and implementation. (AP is\n",
      "PREFACE\n",
      "xv\n",
      "\n",
      "Chunk 5:\n",
      "¬à Emphasis on vocabulary.\n",
      "We try to introduce the minimum number\n",
      "of terms and deÔ¨Åne them carefully when they are Ô¨Årst used. We also\n",
      "organize them in glossaries at the end of each chapter.\n",
      "¬à Program development. There are many strategies for writing programs,\n",
      "including bottom-up, top-down, and others. We demonstrate multiple\n",
      "program development techniques, allowing readers to choose methods\n",
      "that work best for them.\n",
      "¬à Multiple learning curves. To write a program, you have to understand\n",
      "the algorithm, know the programming language, and be able to debug\n",
      "errors. We discuss these and other aspects throughout the book, and\n",
      "include an appendix that summarizes our advice.\n",
      "Object-oriented programming\n",
      "Some Java books introduce classes and objects immediately; others begin with\n",
      "procedural programming and transition to object-oriented more gradually.\n",
      "Many of Java‚Äôs object-oriented features are motivated by problems with previ-\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=5):\n",
    "    # Encode the query into an embedding\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "\n",
    "    # Query the Chroma collection\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        include=['documents', 'metadatas']\n",
    "    )\n",
    "\n",
    "    # Return documents and metadata\n",
    "    return results['documents'][0], results['metadatas'][0]\n",
    "\n",
    "# Example usage\n",
    "query = \"What is object-oriented programming?\"\n",
    "docs, meta = retrieve_relevant_chunks(query)\n",
    "\n",
    "# Print results\n",
    "for i, d in enumerate(docs):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baa3a1ce-cb87-4502-90e2-e8afe83e4a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Answer:\n",
      "  A class is a blueprint or template for creating objects, which defines the attributes (properties) and methods (functions) that the objects of that class will have. An object is an instance of a class, which means it is a specific realization of the class with its own unique values for the attributes defined in the class. In other words, a class is a kind of \"recipe\" for making multiple objects, while an object is a concrete example of that recipe.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def generate_answer_mistral(query, context_chunks):\n",
    "    # Combine context chunks into a single string\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # Construct the prompt\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Generate response using Ollama\n",
    "    response = ollama.chat(\n",
    "        model='mistral',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    # Return only the answer content\n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the difference between a class and an object?\"\n",
    "chunks, _ = retrieve_relevant_chunks(query)\n",
    "answer = generate_answer_mistral(query, chunks)\n",
    "\n",
    "print(\"\\nüìò Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e5d33f-7fb8-40a1-9a66-fa7db761a7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Answer:\n",
      "  The text discusses two sorting algorithms: selection sort and merge sort. Selection sort is mentioned in Exercise 13.1, where it's recommended to learn more about the algorithms at http://www.sorting-algorithms.com/. It is also described as a simple algorithm that sorts n items by traversing the array n-1 times, with each traversal taking time proportional to n, resulting in a total time proportional to n^2.\n",
      "\n",
      "Merge sort, on the other hand, is introduced in Section 13.4 and is presented as a more efficient alternative to selection sort. To sort n items, merge sort takes time proportional to n log2 n, which can be significantly faster than n^2 for larger values of n. The implementation details for both algorithms are provided later in the chapter.\n",
      "\n",
      "üîñ Sources:\n",
      "‚Ä¢ Unknown\n",
      "‚Ä¢ thinkjava\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_chunks(query, k=3):\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k\n",
    "    )\n",
    "    \n",
    "    texts = results['documents'][0]\n",
    "    metadatas = results['metadatas'][0]\n",
    "    \n",
    "    return texts, metadatas\n",
    "\n",
    "\n",
    "# Input query\n",
    "query = \"How do we sort an array?\"\n",
    "\n",
    "# Retrieve chunks and metadata\n",
    "chunks, metadatas = retrieve_relevant_chunks(query)\n",
    "\n",
    "# Generate answer\n",
    "answer = generate_answer_mistral(query, chunks)\n",
    "\n",
    "# Show answer\n",
    "print(\"\\nüìò Answer:\\n\", answer)\n",
    "\n",
    "# Show unique sources from metadata\n",
    "unique_sources = set(meta.get('source', 'Unknown') for meta in metadatas)\n",
    "\n",
    "print(\"\\nüîñ Sources:\")\n",
    "for source in unique_sources:\n",
    "    print(f\"‚Ä¢ {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d0f21c-a853-4e13-8ef6-69eff86d53d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ File selected: \n"
     ]
    }
   ],
   "source": [
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "# File picker (PDF or TXT)\n",
    "Tk().withdraw()  # Hide the Tkinter root window\n",
    "file_path = askopenfilename(\n",
    "    title=\"Select your PDF or Text file\",\n",
    "    filetypes=[(\"PDF files\", \"*.pdf\"), (\"Text files\", \"*.txt\")]\n",
    ")\n",
    "print(\"üìÇ File selected:\", file_path)\n",
    "\n",
    "# Text splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pdfplumber\n",
    "import os\n",
    "\n",
    "def read_pdf_and_chunk(file_path, chunk_size=200, chunk_overlap=20):\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        raise ValueError(\"Only PDF files are currently supported in this function.\")\n",
    "\n",
    "    raw_text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                raw_text += text + \"\\n\"\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Skipped empty page {i+1}\")\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_text(raw_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72bba153-4ce1-4455-9a82-8a9c70c0da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "def embed_chunks(chunks, source_name=\"unknown_source\"):\n",
    "    if not chunks or not isinstance(chunks, list):\n",
    "        raise ValueError(\"Chunks must be a non-empty list of text segments.\")\n",
    "\n",
    "    # Initialize the embedding model only once\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Generate metadata for each chunk\n",
    "    metadatas = [{\"source\": source_name} for _ in chunks]\n",
    "\n",
    "    # Create a unique folder to avoid overwriting previous vector stores\n",
    "    persist_directory = os.path.join(\"chroma_dbs\", f\"chroma_{uuid.uuid4().hex[:8]}\")\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "    vectordb = Chroma.from_texts(\n",
    "        texts=chunks,\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "    return vectordb, persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6912430-6cb9-4f09-8c2a-d8a76dbd5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "def ask_question(vectordb, question, k=2, max_tokens=256, temperature=0.5):\n",
    "    if not question or not isinstance(question, str):\n",
    "        raise ValueError(\"Question must be a non-empty string.\")\n",
    "    \n",
    "    # Step 1: Create retriever from the vector store\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    # Step 2: Load the LLM (Mistral 7B Instruct)\n",
    "    llm = CTransformers(\n",
    "        model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\",\n",
    "        model_type=\"mistral\",\n",
    "        config={\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Step 3: Create the QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    # Step 4: Run the query\n",
    "    result = qa_chain(question)\n",
    "\n",
    "    # Step 5: Extract and deduplicate source names\n",
    "    cited_sources = list(set(\n",
    "        doc.metadata.get(\"source\", \"unknown\") for doc in result[\"source_documents\"]\n",
    "    ))\n",
    "\n",
    "    return result[\"result\"], cited_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a3f2bba-1131-479b-b676-b83b71a03cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_pdf_and_chunk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Read and chunk the uploaded PDF\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mread_pdf_and_chunk\u001b[49m(file_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Create vector database from the chunks\u001b[39;00m\n\u001b[1;32m      5\u001b[0m vectordb, _ \u001b[38;5;241m=\u001b[39m embed_chunks(chunks, source_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserUpload_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_pdf_and_chunk' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Read and chunk the uploaded PDF\n",
    "chunks = read_pdf_and_chunk(file_path)\n",
    "\n",
    "# Step 2: Create vector database from the chunks\n",
    "vectordb, _ = embed_chunks(chunks, source_name=\"UserUpload_1\")\n",
    "\n",
    "# Step 3: Ask a default question about the uploaded document\n",
    "question = \"\"\n",
    "answer, sources = ask_question(vectordb, question)\n",
    "\n",
    "# Step 4: Print the results\n",
    "print(\"üìò Answer:\", answer)\n",
    "print(\"üîñ Sources:\", sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f11cdce9-b9b5-442a-a214-c9be71e7f873",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_pdf_and_chunk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Read and chunk the uploaded PDF\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mread_pdf_and_chunk\u001b[49m(file_path)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Create vector database from the chunks\u001b[39;00m\n\u001b[1;32m      5\u001b[0m vectordb, _ \u001b[38;5;241m=\u001b[39m embed_chunks(chunks, source_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserUpload_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_pdf_and_chunk' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Read and chunk the uploaded PDF\n",
    "chunks = read_pdf_and_chunk(file_path)\n",
    "\n",
    "# Step 2: Create vector database from the chunks\n",
    "vectordb, _ = embed_chunks(chunks, source_name=\"UserUpload_1\")\n",
    "\n",
    "# Step 3: Ask a default question about the uploaded document\n",
    "question = \"?\"\n",
    "answer, sources = ask_question(vectordb, question)\n",
    "\n",
    "# Step 4: Print the results\n",
    "print(\"üìò Answer:\", answer)\n",
    "print(\"üîñ Sources:\", sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6231321-7dfa-4145-9f23-333468b2b132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed8199-f98a-43a1-bab1-0044003602c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
